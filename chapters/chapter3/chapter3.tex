\chapter{Framework Architecture}
The repro-tools \todo{name subject to change} is a framework developed for analysing the reproducibility issues occuring in the neuroimages. Though this framework is developed with neuroimaging in focus, it can be used to analyze files belonging to any discipline. In this particular usecase, the HCP data containing the neuroimaging data processed under different conditions is analyzed. The term `condition` here refers to different operating system on which the processing takes place. Under an ideal scenario, different HCP subjects, irrespective of the condition under which the processing takes place, are supposed to have identical checksum. Repro-tool helps in identifying the files having differences by comparing the checksum values. It can also quantify these differences with the help of metrics. Repro-tool can also trace the provenance of these differences with the help of data captured by Reprozip.

\begin{center}
\includegraphics[width=\linewidth]{framework_architecture.jpg}
\captionof{figure}{Framework Architecture}
\label{fig:framework_architecture}
\end{center}

  \begin{itemize}
  \item Collecting data from the Human Connectome Preprocessing Pipelines~\cite{DBHumanConnectome}
  \item Creation of docker images containing the HCP Preprocessing pipelines
  \item Deploying the docker images with the help of Boutiques (A cross-platform application repository for data-analysis platforms) to CBRAIN
  \item Processing of HCP data set using pipelines and measures to check file corruption, recording the software and hardware specifications, provenance capture
  \item Analysis of processed HCP data
  \end{itemize}

\section{Pipeline Containerization}
A container is a self-contained, ready-to-use software component with all the necessary dependencies and softwares~\cite{7158965}. Figure~\ref{fig:container_architecture} illustrates the container image architecture. The architecture is based on LXC which makes use of  cgroups and namespaces. The images are layered on top of each other and the writable container image is kept at the top. The top layer is executable and it can have a state. The container can be considered as a directory containing everything needed for exectuion of an application~\cite{7158965}.\\

\begin{center}
   \includegraphics[width=\linewidth]{container_architecture_unedit.png}
   \captionof{figure}{Container Architecture}
   \label{fig:container_architecture}
\end{center}

Namespace isolation prevents processes from seeing resources allocated to each other. Container technologies use seperate namespaces dedicated for each functionality, such as, process isolation, network interfaces, interprocess communication etc. Control groups manage and limit resource access for process groups through limit enforcement, accounting and isolation. Thus namespace and cgroups makes it easier to manage and execute multitenant containers on the host system.

We conducted our study using Docker containers. A docker image is made up of file systems layered on top of each other using AuFS. The underlying technology is LXC containers. In a traditional Linux boot mechanism, the kernel first mounts the rootfs as read-only, makes the integrity check and changes the rootfs volume permission to read-write mode. Rootfs is, "a simple file system that exports Linux's disk caching mechanisms as a dynamically resizable RAM-based filesystem"~\cite{Rootfs}. But in a Docker container, instead of converting the rootfs to read-write mode, it uses a writable file system as illustrated in~\ref{fig:container_architecture}. 

\subsection{Docker images}
The study of neuroimaging pipelines is conducted on CentOS\footnote{\url{https://en.wikipedia.org/wiki/CentOS}} (Community Enterprise Operating System). According to~\cite{CentOS}, ``CentOS is for people who need an enterprise class operating system stability without the cost of certification and support". CentOS operating system is known for its rock-solid reliability, safety, portability, openness and the long product lifecycle~\cite{5665431}. All these qualities make CentOS a popular choice among the research community and
high performance computing clusters. We conducted our study across the below listed CentOS operating system versions.

\begin{itemize}
  \item Linux CentOS 5.11
  \item Linux CentOS 6.8
  \item Linux CentOS 7.2.1511
\end{itemize}

Docker containers were created out of the base version images listed above. These containers were installed with all the necessary softwares and libraries required to run the HCP pipelines. HCP pipelines prerequisites are listed below,

\begin{itemize}
 \item A 64-bit Linux Operating System
 \item FSL Version - 5.0.6
 \item FreeSurfer Version - 5.3.0-HCP
 \item Connectome Workbench Version - 1.0
 \item HCP version of gradunwarp version 1.0.2 (this is optional and needs to be installed only if gradient nonlinearity corrections needs to be done)
\end{itemize}

We used CentOS 5.11, CentOS 6.8 and CentOS 7.2.1511, 64 bit operating system versions for our study. FSL 5.0.6\footnote{\url{https://fsl.fmrib.ox.ac.uk/fsldownloads/oldversions/}}, FreeSurfer 5.3.0-HCP\footnote{\url{http://ftp.nmr.mgh.harvard.edu/pub/dist/freesurfer/5.3.0-HCP/}} and Connectome Workbench 1.0\footnote{\url{https://www.humanconnectome.org/software/get-connectome-workbench}} were installed in the Docker images.

\subsubsection{HCP Pipelines (v3.19.0)}
%Intro to HCP preprocessing 
HCP preprocessing pipelines are designed to minimize the amount of information removed from the HCP data. Figure \ref{fig:hcp_pipelines_overview} illustrates the HCP Preprocessing Pipelines overview. HCP pipelines consists of three structural pipelines (PreFreeSurfer, FreeSurfer and PostFreeSurfer), two functional pipelines (fMRIVolume and fMRISurface) and a Diffusion PreProcessing pipeline~\cite{Gla13}. According to~\cite{Gla13}, the overall goals of HCP Preprocessing pipelines are, ``1) to remove spatial artifacts and distortions; 2) to generate cortical surfaces, segmentations, and myelin maps; 3) to make the data easily viewable in the Connectome Workbench visualization software; 4) to generate precise within-subject cross-modal registrations; 5) to handle surface and volume cross-subject registrations to standard volume and surface spaces; and 6) to make the data available in the Connectivity Informatics Technology Initiative (CIFTI) format in a standard ``grayordinate" space". Grayordinate represents the gray matter in the brain using a surface vertex or a volume voxel~\cite{Grayordinate}. The minimal preprocessed subjects are available in standard format which makes it easier to compare it with subjects from other studies. This standardization makes it easier for researchers to report their findings and replicate the studies.

\begin{center}
   \includegraphics[width=\linewidth]{hcp_preprocessing_overview}
   \captionof{figure}{HCP Preprocessing Pipelines Overview}
   \label{fig:hcp_pipelines_overview}
   \caption*{Extracted from \cite{Gla13}}
\end{center}

Out of the six pipelines illustrated in Figure \ref{fig:hcp_pipelines_overview}, we used three structural pipelines and one functional pipeline for study. The pipelines are listed below, 
\begin{itemize}
  \item PreFreeSurfer
  \item FreesSurfer
  \item PostFreeSurfer
  \item fMRIVolume
\end{itemize}

The HCP pipeline is open source, and we used the version 3.19.0\footnote{\url{https://github.com/Washington-University/Pipelines/releases/tag/v3.19.0}} for our study. The details of pipeines are listed below. 
PreFreeSurfer, FreeSurfer, PostFreeSurfer and fMRIVolume diagrams.

\subsubsection{PreFreeSurfer}
Main goals of, PreFreeSurfer, according to~\cite{Gla13} are, 1) to produce an undistorted ``native" structural volume space for each subject; 2) align the T1W and T2W images; 3) perform bias field correction; 4) register the subject's native structural volume space to MNI space. The workflow of PreFreeSurfer is illustrated in Figure \ref{fig:prefreesurfer_overview}.

\begin{center}
  \includegraphics[width=\linewidth]{PreFreeSurfer_Architecture}
  \captionof{figure}{PreFreeSurfer Overview}
  \label{fig:prefreesurfer_overview}
  \caption*{Extracted from \cite{Gla13}}
\end{center}

\subsubsection{FreeSurfer}
HCP pipelines uses FreeSurfer version 5.2 with a lot of enhancements made particulariy focusing HCP data. The main goals, according to~\cite{Gla13} are, 1) improve the robustness of brain extraction; 2) fine tune T2w to T1w registration; 3) accurately place the white and pial surface with high resolution data; 4) perform FreeSurfer's standard folding-based surface registration to their surface atlas (fsaverage). The workflow of FreeSurfer is illustrated in Figure~\ref{fig:freesurfer_overview}.

\begin{center}
  \includegraphics[width=\linewidth]{FreeSurfer_Architecture}
  \captionof{figure}{FreeSurfer Overview}
  \label{fig:freesurfer_overview}
  \caption*{Extracted from \cite{Gla13}}
\end{center}

\subsubsection{PostFreeSurfer}
Main goals of, PostFreeSurfer, according to~\cite{Gla13} are, 1) produces all of the NIFTI volume and GIFTI surface files necessary for viewing the data in Connectome Workbench,2) application of surface registration; According to \cite{DBLP:journals/corr/HrgeticP13}, ''task of the surface registration algorithms is to determine the corresponding surface parts in the pair of observed clouds of 3D points, and on that basis to determine the spatial translation and rotation between the two views"; 3) downsampling registered surfaces for connectivity analyses; 4) creating the final brain mask, and creating myelin maps. Figure \ref{fig:postfreesurfer_overview} illustrates the workflow of PostFreeSurfer.\\

\begin{center}
  \includegraphics[width=\linewidth]{PostFreeSurfer_Overview}
  \captionof{figure}{PostFreeSurfer Overview}
  \label{fig:postfreesurfer_overview}
  \caption*{Extracted from \cite{Gla13}}
\end{center}

\subsubsection{fMRIVolume}
Main goals of, fMRIVolume, according to~\cite{Gla13} are, 1) remove spatial distortions; 2) realignment of volumes to adjust subject motion; 3) registration of fMRI data to structural volume; 4) normalizaton of the 4D image; 5) mask the data with final brain mask. Figure \ref{fig:fMRIVolume_overview} illustrates the workflow of fMRIVolume pipeline.\\

\begin{center}
  \includegraphics[width=\linewidth]{fMRI_Volume}
  \captionof{figure}{fMRI Volume Overview}
  \label{fig:fMRIVolume_overview}
  \caption*{Extracted from \cite{Gla13}}
\end{center}

\section{Pipeline Encapsulation}
A wrapper script\footnote{\url{https://github.com/big-data-lab-team/Dockerfiles-HCP-PreFreesurfer/blob/master/PreFreeSurfer-DockerFiles/command-line-script.sh}} was written on top of the HCP Pipelines to have the features which are listed below. 
\begin{itemize}
  \item Computes the checksum of the files in each subject before and after the execution
  \item Creates execution directory and copies the subject to prevent corrupting the input data
  \item Records all the software (library versions) present in the container and hardware specifications of the workstation
  \item Optional feature to trace the execution using Reprozip
\end{itemize}

The checksums were computed before and after execution so that we can crosscheck them under different conditions to understand if there are any differences. These checksums are the primary criterion we use to find out if the files are having differeces. The execution directory creation copies the subject folder and thus it prevents the corruption of the HCP data. Another feature, recording of the software library versions and hardware specifications are done in order to make sure that the only factor that changes in these experiments is the operating system version. The optional Reprozip tracing feature is used to record the details of the HCP Processing.

\section{Pipeline Deployment}
The Docker container containing the HCP Pipeline along with the Boutiques descriptor was deployed on a server setup as part of our study. With the help of CBRAIN along with the containers and the descriptors, we were able to process HCP subjects. Single server was used in order to prevent differences occuring in the files due to differences arising from the hardware architecture. Boutiques descriptors used for deploying the HCP Pipelines on CBRAIN is available at \cite{HCP_descriptors}.

%\begin{itemize}
 % \item Single server to ensure no hardware differences
  %\item Able to process multiple subjects at once
  %\item CBRAIN HCP plugins available here
%\end{itemize}

\section{File comparisions across conditions}
Repro-tools framework compares the files across different conditions based on checksum of files. HCP subjects processed under different conditions are kept in different directories. The basic check is making sure that the files are common to all conditions for comparision. A pair of condition is taken at a time for comparison. The checksum of the files are recorded before and after the processing. Repro-tools can compute the checksums locally as well. 

Two types of differences can occur due to the minimal preprocessing. One is inter-OS difference which occur due the the operating system library updates and the other type, intra-OS differences occur due to pseudo-random processes used in the pipelines. Repro-tools can be used to identify both kind of differences.

The first step is identification of files with differences in their checksums. For the files that are identified to have differences, different kind of metrics are used base on the file type to quantify the differences. Normalized root mean square error, Dice Coefficient, Text filter etc. are the various metrics used for quantifying the differences. Section \ref{sec:num1} explains the metrics in detail. 

These metric values help us understand how big or small the differences are. Apart from quanitfying the differences using type specific metrics, Repro-tools can also be used to trace the provenance of these differences. It is able to identify all the processes and associated parameters. These detailed information is helpful in debugging since we can recreate the processing step by step and identify the processes that creates the differences.

%\begin{itemize}
 %\item Identify the files that are common to all subjects and all conditions
 %\item Check intra-OS variability (e.g., due to pseudo-random processes)
 %\item Identifies files with differences (based on md5 checksum)
 %\item Associates file types with specific metrics (Normalized RMSE, Text filters, Dice coefficient)
 %\item Computes difference matrix for each metric and associates matrix with Reprozip trace
%\end{itemize}

\section{Provenance Capture}
According to~\cite{Ikeda:2010:PSP:1855795.1855800}, ``provenance captures where data came from, how it was derived, manipulated, and combined, and how it has been updated over time". Provenance information can be used to explain the source or evolution of a data set. Thus it can help in generating a deeper understanding of the data set. It can also be used to verify and confirm that there were no bugs in the processing. Provenance information can be used to identify the bugs in the processing. Thus it can help in recomputing the steps that got corrupted and send the corrected data downstream~\cite{Ikeda:2010:PSP:1855795.1855800}.

The data recorded by Reprozip is recorded in an SQLite\footnote{\url{https://www.sqlite.org/}} database. Repro-tools framework queries this database to find out the provenance information about the files that has a difference in their checksum. Figure \ref{fig:provenance-query} contains the query used for finding the provenance information.\\

\begin{tcolorbox}[colback=black!5!white,colframe=black!75!black]
SELECT DISTINCT executed\_files.name, executed\_files.argv, executed\_files.envp, executed\_files.timestamp, executed\_files.workingdir from executed\_files INNER JOIN opened\_files where opened\_files.process = executed\_files.process and opened\_files.name like ? and opened\_files.mode=2 and opened\_files.is\_directory=0',('\%/'+file\_name,)
\end{tcolorbox}
\captionof{figure}{Query for provenance information}
\label{fig:provenance-query}

Query makes use of details about the 1) processes that wrote the file; 2) command line arguments used by those processes; 3) Environment variables used; 4) timstamp; 5) working directory. By the help of the details we get out of the query, we can reproduce the steps to debug the processes that create these differences.

\section{Metrics} \label{sec:num1}
The metrics those are used to quantify the differences are described below.

\subsection{Normalized Root Mean Sqaure Error (NRMSE)}
As defined in \cite{khosrow2017handbook}, ``The Root Mean Square Deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the difference between values predicted by a model or an estimator and the values actually observed". Normalizing the RMSD value makes it easier to compare between models or datasets.

The RMSE of predicted values ${\displaystyle {\hat {y}}_{t}}$  for times t of a regression's dependent variable ${\displaystyle y_{t}}$ is computed for n different predictions as the square root of the mean of the squares of the deviations:\\
\begin{center}
  \begin{equation}
     RMSE = {\sqrt {\frac{1} {n}{\sum\limits_{t = 1}^n {(\hat{y}_{t} - {y}_{t} } })^{2} } }
  \end{equation}
\end{center}

\begin{center}
  \begin{equation}
    NRMSE = {\frac{RMSE} {y_{max} - y_{min}}}
  \end{equation}
\end{center}

\begin{itemize}
\item NRMSE
\item Checksum
\item Dice
\item Hardware Differences
\end{itemize}
