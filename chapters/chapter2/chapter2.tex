\chapter{Related Work}

\section{Reproducibility of neuroimaging pipelines across operating systems}

%Summarize both papers and add the summary. 
The study \cite{Gla15}, on FSL\footnote{\url{https://fsl.fmrib.ox.ac.uk/fsl/fslwiki}}, Freesurfer\footnote{\url{https://surfer.nmr.mgh.harvard.edu/}} and CIVET\footnote{\url{http://www.bic.mni.mcgill.ca/ServicesSoftware/CIVET}} and different versions of GNU/Linux found that the differences are occuring due to the evolution of math libraries used in the operating system. As illustrated in Figure \ref{fig:Types of builds}, ``the execution of an application depends on its source code, on the compilation process, on software libraries, on an operating system (OS) kernel, and on a hardware processor. Libraries may be embedded in the application, i.e., statically linked, or loaded from the OS, i.e., dynamically linked. The reproducibility of results may be influenced by any variation in these elements, in particular: versions of the source code, compilation options, versions of the dynamic and static libraries (in particular when these libraries implement mathematical functions), or architecture of hardware systems" ~\cite{Gla15}. \\  

\begin{center}
\includegraphics[width=\linewidth]{builds.jpg}
\captionof{figure}{Source code, compilation, libraries, kernel and hardware}
\label{fig:Types of builds}
\caption*{Extracted from \cite{Gla15}}
\end{center}

%Gronenschild study summary
A similiar study \cite{10.1371/journal.pone.0038234} was conducted on FreeSurfer (A popular and freely available set of tools for Neuroimage processing) and the main focus was to identify the variabilities resulting from different data processing conditions like software version, operating system and hardware. This study identified differences in images due to updates in the operating system versions (OSX 10.6 and OSX 10.5) as well as FreeSurfer (v4.3.1 vs. v4.5.0, v4.3.1 vs. v5.0.0, and v4.5.0 vs. v5.0.0) software. They also identified differences in the images when the images were processed on different set of hardware (Mac vs HP systems). Study concludes that ``users are discouraged to update to a new major release of either FreeSurfer or operating system or to switch to a different type of workstation without repeating the analysis".\\

The above studies points out the differences that can arise due to the differences in its source code, compilation process, software library version, kernel version or the architecture of the hardware it uses for execution. With the use of containers, these reproducibility issues can be minimized upto an extent but still the host operating system dependency can influence the output of an application since it might use the underlying operating system libraries from host machine.

\section{Containers}
%Containers intro
Containers allow a user to run an application and its dependencies in resource-isolated processes by encapsulating the entire software environment. Containers thus help in tackling one of the serious challenges associated with making an experiment computationally reproducible, i.e., the rapidly changing nature of computer environments. \\

%2 History of containers
Container technologies has been around for a while and there are numerous implementations of it. In the year 2000, FreeBSD (4.0) featured the Jails system which focused on providing a virtual environment running on the host machine with its own files, processes, user and superuser accounts. Later came OpenSolaris, providing not only isolation services but also mechanisms related to snapshots (A snapshot is a read-only copy of a file system or volume)  and cloning (A clone is a writable volume or file system whose initial contents are the same as the dataset from which it was created and clones can be created only from snapshots). In 2005, OpenVZ was announced as a containerization technology supporting Linux systems. Linux containers (LXC), took advantage of the namespace concept. LXC extended the isolation property to users, processes and networking. In 2006, Google started a project which implemented a functionality to limit the resource usage of containers. This project was later merged into the Linux kernel and it was named as "cgroups" feature. ``Control cgroups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups whose usage of various types of resources can then be limited and monitored" \cite{cgroups}. Docker, was started as an open source project in 2013, which basically added an additional layer on top of Linux Containers (LXC), exposing additional features such as mounted storage, network port redirection, and container catalog management. Singularity, was started in 2015, with main focus on experimental reproducbility and isolation \cite{Xavier:2013:PEC:2497369.2497577}.\\

%Different kinds of virtualization
Full virtualization, paravirtualization and container based are different kinds of virtualization. In full virtualization, guest operating systems can run without modifications on top of the host operating system and the virtual machine monitor (VMM) \cite{7382987}. ``Hypervisor also known as Virtual Machine Monitor, is the basic software for providing virtualization. Virtualization creates the environment for various operating system to run on a single node or host computer or machine. The main purpose of the hypervisor (VMM) is to monitor the Virtual Machine (VM) that runs above the VMM. This enables more than one guest machine to utilize the hardware resources of a single host machine" \cite{hypervisor}. In paravirtualization, the guest OS (the one being virtualized) is aware that it is in a virtualized environment and the guest's kernel is modified to communicate directly with the virtualization hyeprvisor \cite{7382987}. OS-Layer virtualization or container-based virtualization differs from full or paravirtualization by modifying the underlying OS to isolate instances. It utilizes host OS kernel to provide isolation and multi-tenancy layer.

\begin{center}
\includegraphics[width=\linewidth]{cont-vm.png}
\captionof{figure}{Comparison of hypervisor (a) and container (b) â€”based instances.}
\label{fig:Comparison of container vs. hypervisor}
\caption*{Extracted from \cite{7382987}}
\end{center}

%Differences container and virtual machine 
Figure \ref{fig:Comparison of container vs. hypervisor} illustrates the differences based on hypervisor and container. Containers have a closer access to operating system services than their counterpart virtualization tools which makes their performance closer to the performance exhibited on top of native environments \cite{Xavier:2013:PEC:2497369.2497577}. Containers accesses the host operating system features directly making it lightweight. ``Docker" and ``Singularity" are two container based technologies widely used for containerizing the applications.\\

% Features of containers
Some of the notable features of containers \cite{docker-run} \cite{DBLP:journals/corr/HaleLRW16} \cite{Julian:2016:CRI:2949550.2949562} \cite{10.1109/ISPASS.2015.7095802} which facilitate conducting experiments easily are listed below:

\begin{itemize}
  \item Helps in encapsulating the entire software environment
  \item Avoids the software version conflicts with the host os by packaging the right software versions and dependencies in the container environment
  \item Simplified collaboration and sharing ability
  \item Improved reproducibility and portability of applications
  \item Host OS agnostic
  \item Rapid deployment and execution at scale
\end{itemize}


What are its limitations
Include the problems listed in link https://goo.gl/zodrok

\subsection{Docker}
%Docker intro
As illustrated in Figure \ref{fig:docker_architecture}, Docker uses a client-server architecture. The Docker software runs as a daemon on host machine. This daemon can launch containers, control their isolation level, monitor them to trigger actions, and spawn shells into running containers for administration purposes. Daemon can change iptable rules on the host and create network interfaces. Docker can create and store images encapsulating an entire software environment. Docker images can be stored locally or it can be stored in Dockerhub\footnote{\url{https://hub.docker.com/}}. Dockerhub is an online repository that helps developers to manage the Docker images. Anyone who sign up on the Dockerhub has access to public images and they can also host their own images. There is also feature to automate the image creation with the help of Git\footnote{\url{https://git-scm.com/}}. Any user can create repository under his account and the repositories are namespaced using the format, "\textless developer\textgreater/\textless repository\textgreater" \cite{7742298}. The management of the images on the host machine, pushing and pulling of images from Dockerhub\footnote{\url{https://www.hub.docker.com/}}, building images from Dockerfile are all taken care by the daemon. The daemon itself runs as a root user on the host machine and it is remotely controlled through a Unix socket. The Docker client talks to the Docker daemon and the daemon does all the heavy lifting like pushing, pulling and building images.The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. The client and daemon need not necessarily be on the same machine \cite{docker-documentation}.

\begin{center}
\includegraphics[width=\linewidth]{docker_architecture.png}
\captionof{figure}{Docker Architecture}
\label{fig:docker_architecture}
\caption*{Extracted from \cite{docker-documentation}}
\end{center}

%Docker Features
Docker provides access to virtualization facilities provided by the Linux kernel, along with some abstracted virtualized interfaces such as libvirt\footnote{\url{https://libvirt.org/}}, LXC and systemd-nspawn\footnote{\url{https://www.freedesktop.org/software/systemd/man/systemdnspawn.html}}. The control over the host's resources is provided thorugh Control Groups (cgroups) and thus it limits the amount of resources used by a container such as memory, diskspace and I/O. Docker features a layered filesystem called AuFS (Advanced Multi Layered Unification Filesystem) which allows to overlay one or more existing filesystems. This AuFS feature provides capabilites such as image versioning management and exposing base images to more specialized virtualized systems. One of the main reason for the wide adoption of Docker containers is that it can leverage the infrastructure consolidation (an organization's strategy to reduce IT assets by using more efficient technologies) and it exhibits a low resource footprint. Docker also boosted the adoption of service oriented architectures (e.g. microservices) because that makes the deployment of self-contained modules easy. They are also able to independantly interact with third parties using exising network protocols (e.g. web services) \cite{Xavier:2013:PEC:2497369.2497577}. \\

%How it can help the experiments
The development of standardized Dockerfile format for describing and managing software containers is very straightforward. Docker helps developers to easily create standard containers for their software applications or services. For a system administrator, Docker helps in the automation of depolyment and management of business level services with the help of containers. Docker can be used as a part of virtualization layer for deploying and managing the execution environments. Another advantage is that Docker containers provide reliable and predicatable execution environments and thus helps in reducing the issues related to deployment \cite{DBLP:journals/corr/MorrisVHM17}.

\subsection{Singularity}
%Singularity intro
``Singularity is an open source initiative that harnesses the expertise of system and software engineers and researchers alike, and integrates seamlessly into common workflows for both of these groups. One of the major reason for this this open source initiative started by the Lawrence Berkeley National Laboratory (LBNL) was that, if Docker is used on HPC environments, that would mean an unreasonable level of security risk. So Docker could not be used by a large group of people that needed it greatly. Thus Singularity open source initiative came up with a product that could be used across academia and industry which is agnostic to the environment. It was developed in collaboration by HPC administrators, developers and research scientists alike. The main issue with docker running on the HPC clusters was that the daemon has to run as a root user which could lead to unnecessary risks like coercing the daemon process into granting the users escalated privileges" \cite{10.1371/journal.pone.0177459}.\\

\begin{center}
\includegraphics[width=\linewidth]{singularity.png}
\captionof{figure}{Singularity usage workflow.}
\label{fig:singularity_workflow}
\caption*{Extracted from \cite{10.1371/journal.pone.0177459}}
\end{center}
%Singularity feature

The main goals of Singularity are, 1)Mobility of compute 2)Reproducibility 3)User Freedom 4)Support on existing traditional HPC resources. The mobility of compute is the ability to create and maintian a workflow locally and being able to run the same worklfow across different hosts, operating systems or computing platforms without any problems or tweaks. Singularity achieves this by utilizing a distributable image format that encapsulates the entire container and stack into a single image file. The feature that supports reproducibility is the use of hashing. Any singularity image can make use of the hash feature to create hash and store it as metadata with built images. Users can verify these hashes to check if the image is modified or not. User freedom is granted by the ability to define their own working environment and copy the Singularity image containing the entire details of that environment along with the code to a shared resource and reproduce the workflow inside that image. Singularity supports the existing and traditional HPC resources easily as installing a single package onto host operating system. Singularity is compatible with RHEL and Linux distributions dating back to Linux2.2. It natively supports the resource managers (e.g SLURM\footnote{\url{https://slurm.schedmd.com/}} - a free and open-source job scheduler for Linux and Unix-like kernels, Torque\footnote{\url{http://www.adaptivecomputing.com/products/open-source/torque/}} - an open-source Resource and QUEue Manager is a distributed resource manager providing control over batch jobs and distributed compute nodes, SGE\footnote{\url{https://en.wikipedia.org/wiki/Oracle_Grid_Engine}} - a grid computing computer cluster software system, etc.) and supports technologies such as InfiniBand\footnote{\url{https://en.wikipedia.org/wiki/InfiniBand}}(A computer-networking communications standard used in high-performance computing that features very high throughput and very low latency) and Lustre\footnote{\url{https://en.wikipedia.org/wiki/Lustre_(file_system)}}(Open source, parallel file system that supports many requirements of leadership class HPC simulation environments).\\

%Singularity how it helps
Singularity image encapsulates the operating system environment and all application dependencies necessary to run a defined workflow. Singularity container supports different kinds of uniform resource identifiers (http:// and https://) and also other container formats like Docker (docker:// - for pulling images from docker hub, shub:// - for pulling images from singularity hub). Singularity images thus can be created from existing Docker images since it supports Docker.
 
\subsection{Boutiques}
To enable sharing of ideas and software, it is a common practice to port the applications to common platforms so that the community as a whole is able to make use of the ready-to-use applications. However, porting applications to a high performance computing infrastructure or a web platform is not so easy. Application porting needs considerable effort of manual effort to `` 1) install the application on the target infrastructure, 2) describing the application in a format compatible with the execution platform, and 3) generating proper user interfacesi" \cite{boutiques}. These installations and deployments are platform specific and thus the same tasks has to be repeated from one platform to another. Boutiques, an open source tool, can be used to save time and cost spent on these repeated actions to deploy applications on various platforms. \\

``Docker and Singularity, greatly facilitate the sharing of software and improve the reproducibility of analysis by defining immutable, reusable execution environments" \cite{boutiques}. The software or applications inside these containers should be invoked through proper command line to run an application. Boutiques makes use of a flexible template that could properly describe the inputs and outputs an application produces. ``The formal descriptions, also known as \textit{manifests}, are used for validation of input values to prevent errors. These manifests are intended to be produced by scientific application developers, for publishing them in common repositories and to be consumed by execution platforms". The preferred way of describing these command line template, input and argument is through a JSON document. The manifest contains the details to a container where the intended application is installed. The proper command line argument is built at runtime with the help of maifest and the template gets replaced by the actual values given by the user. For validating the inputs an invocation schema is used. \\

Here is an example of a typical command-line template:

\begin{verbatim}
          exampleTool [INPUT-FILE] [OUTPUT_FILE]
\end{verbatim}

The command line above invokes a tool named exampleTool that needs an two arguments, an input file and an output-folder. ``\textit{Inputs} must have a name, a unique identifier and a type. They may be optional, have a description, a value key, a flag separator, and a default value. Inputs may also be ordered lists: in this case, value keys are substituted by the space-separated list of input values. \textit{Output} files must have a unique identifier, a name and a path template that specifies the file or directory name" \cite{boutiques}.

At runtime, with the help of the JSON descriptor, Boutiques substitutes all the mandatory parameters and the optional parameters with the original values selected by the user. The core tools of Boutiques are validator and local executor. Boutiques validator checks if the JSON manifests confirm to the rules of Boutiques schema. And local executor can be used to test and debug the applications locally.

\begin{center}
\includegraphics[width=\linewidth]{input.png}
\captionof{figure}{Boutiques input descriptor.}
\label{fig:input}
\end{center}

\begin{center}
\includegraphics[width=\linewidth]{output.png}
\captionof{figure}{Boutiques output descriptor.}
\label{fig:output}
\end{center}



\section{Web platforms to run containers}
\subsection{Why do we need these platforms?}
\subsection{Examples of web computing platforms}
\subsubsection{CBRAIN}
%CBRAIN OVerview
The Canadian Brain Imaging Research Platform (CBRAIN), is a web platform developed at the Montreal Neurological Institute (MNI) to tackle the Big-Data research and heavy computational challenges faced by neuroimaging researchers. Even though it is mainly used for neuroimaging, ``the platform is built as a generic framework that can accept data and analysis tools from any discipline" \cite{DBLP:journals/fini/DasGRSPMSRSKMKR17}. CBRAIN offers a controlled and secure platform which is user friendly , lightweight, extensible and it can supports heterogenous computing platforms and data resources. It also provides access to an array of processing and visualization tools. The CBRAIN service deployed at the Montreal Neurological Institute relies on the infrastructure provided by Compute Canada \cite{DAS20161188}. It currently provides 500+ collaborators in 22 countries with web access to several systems, including 6 clusters of the Compute Canada highperformance computing infrastructure (totaling more than 100,000 computing cores and 40 PB of disk storage) and Amazon EC2. CBRAIN transiently stores about 10 million files representing over 50 TB distributed in 42 servers. 51 public data processing applications are integrated and over 340,000 processing batches have been submitted since 2010.

%CBRAIN Architecture brief intro
The three main layers of CBRAIN are, `` (i)the access layer, accessible through a standard web-browser (for users) or a RESTful WebAPI (for applications or other platforms),(ii) the service layer which provides portal services for the access layer, the metadata database, which stores information about all users, permissions, and resources, and orchestration services for resource coordination (users requests, data movement, computing loads, jobs, data models,...), and finally (iii) an infrastructure layer consisting of networked data repositories and computing resources" \cite{DBLP:journals/fini/DasGRSPMSRSKMKR17}.

%CBRAIN support boutiques
 
\subsubsection{Amazon}

\section{Interposition Techniques}
\subsection{System call interception}
\subsection{Library call interception}
\subsection{Reprozip tool}
\hyperref[System Call Interception]{http://landley.net/kdocs/mirror/ols2007v1.pdf}

\section{NeuroImaging Pipelines}
%What is neuroimagaing
Neuroimaging pipelines are used for the analysis and visualization of human brain structure, function and connectivity. Neuroscientists are are relying on the neuroimaging techniques as an essential tool for understanding the complex spatial and temporal characteristics of human brain. Some of the popular neuroimaging techniques are  functional MRI\footnote{\url{https://en.wikipedia.org/wiki/Functional_magnetic_resonance_imaging}} (fMRI), diffusion tensor imaging\footnote{\url{https://en.wikipedia.org/wiki/Diffusion_MRI}} (DTI), voxel based morphometry\footnote{\url{https://en.wikipedia.org/wiki/Voxel-based_morphometry}} (VBM), magnetoencephalography \footnote{\url{https://en.wikipedia.org/wiki/Magnetoencephalography}} (MEG), electroencephalography\footnote{\url{https://en.wikipedia.org/wiki/Electroencephalography}} (EEG), and optical imaging. fMRI measures brain activity by detecting changes associated with blood flow. DTI is an MRI-based neuroimaging technique which makes it possible to estimate the location, orientation, and anisotropy (The property of being directionally dependent, which implies different properties in different directions) of the brain's white matter tracts. VMB is a neuroimaging analysis technique that allows investigation of focal differences in brain anatomy, using the statistical approach of statistical parametric mapping. MEG is a functional neuroimaging technique for mapping brain activity by recording magnetic fields produced by electrical currents . EEG is an electrophysiological monitoring method to record electrical activity of the brain. ``Optical imaging involves shining light into a tissue with one or more sources, and measuring light output from the tissue using one or more detectors" \cite{doi:10.1080/23273798.2017.1290810}.\\

The Human Connectome Project(HCP) faces the challenging task of bringing multiple magnetic resonance imaging(MRI) modalities, structural, functional, and diffusion,together into a common automated preprocessing framework across a large cohort of subjects. This framework is open and freely accessible \cite{Gla13}. The dataset from the HCP, are qualitatively different from standard neuroimaging data, having higher spatial and temporal resolutions and differing distortions. So these preprocessing pipelines creates preprocessing results that are available in standard volume and combined surface and volume spaces which makes it easier for researchers to compare the images across the neuroimaging spectrum. Since the images from the HCP dataset are cutting edge in terms of quality, it is anticipated to be widely used.\\

HCP minimal preprocessing pipelines are designed to minimize the amount of information actually removed from the data. These pipelines are dependant on the underlying operating system libraries for many of its functions. Hence, when there are updates in operating system versions, there could be changes in the ways which the rounding off and truncation of floating-point numbers are handled. We are trying to quantify the differences occuring due to the operating system updates on HCP preprocessing pipelines.\\

\begin{itemize}
 \item FSL
 \item FreeSurfer
 \item HCP Pipelines
\end{itemize}


