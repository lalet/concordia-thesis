\chapter{Related Work}

\section{Reproducibility of neuroimaging pipelines across operating systems}
Neuroimaging pipelines are known to generate different results depending on the computing platform where they are compiled and executed \cite{Gla15}. Such reproducibility issues, also known as computing noise, arise from variations in harware architectures and software versions. The state-of-the-art solution to deal with these issues is to restrict studies to a single computing platform (harware and software), which has several drawbacks and, therefore, not feasible. So the best approach is to become aware of these differences.
Study conducted on FSL, Freesurfer and CIVET and different versions of GNU/Linux \cite{Gla15} shows that the differences are occuring due to the evolution of math libraries used in the operating system on which the processing takes place. A similiar study \cite{10.1371/journal.pone.0038234} concludes that the operating system updates and the software updates itself can cause diffrences in the results of neuroimaging pipelines. Results from these studies shows that there is numerical instability in these pipelines which causes reproducibility issues.

Explain differences.

\section{Potential causes of differences}
The issues regarding reproducbility could arise due to a variety of reasons other than the evolution of math libraries. The way in which the build takes place(static vs. dynamic),the nature of the language(compiler vs. interpreter), the version of the compiler used, the options used while compiling the code etc. 

\section{Containers}
Computational reproducibility has become an issue of increasing importance in several research domains.	The serious challenge associated with making an experiment reproducible is the rapidly changing nature of computer enviroments. The commonly used approaches to tackle these issues are workflow softwares and virtual machines.

Workflow softwares provides very elegant technical solutions to the challenges of communication between diverse software tools, capturing provenance in graphically driven interfaces but most workflow systems struggle with relatively low total adoption. Virtual machines offer a more direct approach to the problem of computational reproducibility but critics highlight that it is too much of a black box approach and thus is ill-suited for reproducibility. Also it is impossible to combine multiple studies into a pipeline if each study has its own self contained virtual machine environment\cite{Boettiger:2015:IDR:2723872.2723882}.

A more lightweight solution is used widely among the scientific community thesedays, called containers. Container is similiar to a virtual machine, but it uses the same host operating system along with a containter manager. Containers have a closer access to operating systems than their counterpart virtualization tools such as native virtualization, paravirtualization, and hypervisors. In native virtualization, also known as full virtualization ,the guest OS is fully abstracted (completely decoupled) from the underlying hardware by the virtualization layer. Paravirtualization involves modifying the OS kernel to replace nonvirtualizable instructions with hypercalls(a hypercall is a way for the virtualized operating systems to make the hypervisor handle privileged operations) that communicate directly with the virtualization layer hypervisor \cite{citeulike:11530382}. Hypervisors present a simplified view of the native hardware to the virtual machines then they can barely access to the optimized set of instructions of actual processors.Containers subtract the hypervisor layer of the virtualization equations and relies on namespaces and cgroups in order to provide isolation and accounting of the consumed resources by the container instances.

\begin{center}
\includegraphics[width=0.5\textwidth]{fullvirtualization.png}
\end{center}

These containers allow an user to run application and its dependancies in resource-isolated processes. Containers have a closer access to operating system services than their counterpart virtualization tools which makes their performance closer to the performance exhibited on top of native environments\cite{Xavier:2013:PEC:2497369.2497577}.

How containers help reproducibility
Even after knowing the software versions and tools, it is extremely hard to reproduce the workflow to reproduce an experiment. The operating system type, version and harwdware specifications used etc can have significant effect on the experiment. Containers can help in recreating the software environment used for the experiment in its entirety. This can help the researchers significantly since they don't have to go through the entire process of installation process. "Docker" and "Singularity" are two container based technologies widely used for containerizing the experiments.

With the use of version control technologies(Github, Bitbucket) and Dockerhub, SingularityHub, we can create automated builds which creates images everytime there is a change in the build files. Thus making sure the images contain all the latest changes in it. 
What are its limitations
Include the problems listed in link https://goo.gl/zodrok
\subsection{Docker}
Explain docker, cite paper
\subsection{Singularity}
Explain singularity, cite paper
\subsection{Boutiques}

\section{Web platforms to run containers}
\subsection{Why do we need these platforms?}
\subsection{Examples of web computing platforms}
\subsubsection{CBRAIN}
\subsubsection{Amazon}

\section{Interposition Techniques}
\subsection{System call interception}
\subsection{Library call interception}
\subsection{Reprozip tool}
\hyperref[System Call Interception]{http://landley.net/kdocs/mirror/ols2007v1.pdf}

\section{NeuroImaging Pipelines}
The Human Connectome Project(HCP) faces the challenging task of bringing multiple magnetic resonance imaging(MRI) modalities, structural, functional, and diffusion,together into a common automated preprocessing framework across a large cohort of subjects. This framework is open and freely accessible \cite{Gla13}. The dataset from the HCP, are qualitatively different from standard neuroimaging data, having higher spatial and temporal resolutions and differing distortions. So these preprocessing pipelines creates preprocessing results that are available in standard volume and combined surface and volume spaces which makes it easier for researchers to compare the images across the neuroimaging spectrum. Since the images from the HCP dataset are cutting edge in terms of quality, it is anticipated to be widely used.

HCP minimal preprocessing pipelines are designed to minimize the amount of information actually removed from the data. These pipelines are dependant on the underlying operating system libraries for many of its functions. Hence, when there are updates in operating system versions, there could be changes in the ways which the rounding off and truncation of floating-point numbers are handled. We are trying to quantify the differences occuring due to the operating system updates on HCP preprocessing pipelines.

\begin{itemize}
 \item FSL
 \item FreeSurfer
 \item HCP Pipelines
\end{itemize}


