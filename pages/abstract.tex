\begin{abstract}
The lack of computational reproducibility threatens data science in several domains.
This study focuses on neuroimaging pipelines and aims to (i) identify the effect of the operating system on neuroimaging pipelines (ii) quantify this effect with the use of standard metrics.
The effect of operating systems can create differences in the output images.
Two kinds of differences can occur in the output images, namely inter-OS and inter-run differences.
Inter-OS differences are created due to the differences in the operating system libraries (different OS versions) as a result of various system updates.
Inter-run differences are differences that occur within the same operating system, due to reasons like pseudo-random process, silent crashes etc. in the operating system or in the pipeline.
%framework
To conduct this study, a framework for evaluating the reproducibility of these neuroimaging pipelines was developed. 
The two major functionalities of this framework being, processing of the subjects and the analysis of the processed data. 
The framework consists of Docker containers, CBRAIN web portal and Reprozip for provenance capture (record the system calls and arguments).
A tool (Repro-tool) was developed for the analysis of the results. Normalized root mean square error and
Dice coefficient of similarity were the metrics used for quantifying the differences.
% Experiment description
We applied our reproducibility framework to the evaluation of the effect of the
operating system on results produced by pipelines from the Human
Connectome Project (HCP), a large open-data initiative to study the human brain.
In particular, we focused on pre-processing pipelines for
anatomical and functional data, namely PreFreeSurfer, FreeSurfer,
PostFreeSurfer and fMRIVolume.
We used data from 5 subjects released by
the HCP.
% Results
Results highlight substantial differences in the output of the HCP
pipelines obtained in two versions of Linux (CentOS6 and CentOS7).
Inter-OS differences corresponding to normalized root mean square
errors of up to 0.27 were observed, which corresponds to visually
important differences. We provide visualizations of the most important
differences for various pipeline steps. No meaningful inter-run
differences were observed, which shows that the inter-OS differences do
not originate from the use of pseudo-random numbers or silent crashes
of the pipelines.
% Discussion
We hypothesize that the observed inter-OS differences come from
numerical instabilities in the pipelines, triggered by rounding and
truncation differences that originate in the update of mathematical
libraries in different systems. An apparent solution to this issue
is to freeze the execution environment using, for instance, software containers.
However, this would only mask instabilities while they should ultimately be
corrected in the pipelines.
\end{abstract}
